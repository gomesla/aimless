{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# NOTE: This will fail for large dataset processing or complex model evaluation\n",
    "# Use the command below to run it in the background \n",
    "##############################\n",
    "# Source: https://www.maksimeren.com/post/screen-and-jupyter-a-way-to-run-long-notebooks-headless/\n",
    "# jupyter nbconvert --to notebook --execute used_car_price_analysis.template.ipynb --output=used_car_price_analysis.out.ipynb --ExecutePreprocessor.timeout=-1\n",
    "\n",
    "\n",
    "##############################\n",
    "# Library Imports\n",
    "##############################\n",
    "import copy\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Frameworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn import set_config\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectFromModel, RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_transformer, make_column_selector, TransformedTargetRegressor, ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "\n",
    "# xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "##############################\n",
    "# Runtime Settings\n",
    "##############################\n",
    "set_config(display=\"diagram\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##############################\n",
    "# Flow control + Constants\n",
    "##############################\n",
    "DEV_MODE = False\n",
    "USE_TARGET_ENCODER_FOR_HIGH_CARDINALTY = True\n",
    "MODEL1_ACTIVE = True\n",
    "MODEL2_ACTIVE = True\n",
    "MODEL3_ACTIVE = True\n",
    "MODEL4_ACTIVE = False\n",
    "IMAGE_DIR_SUFFIX = ''\n",
    "\n",
    "\n",
    "# Constants\n",
    "RESULT_DIR = './analysis_results'\n",
    "if DEV_MODE:\n",
    "    RESULT_DIR = RESULT_DIR + '_dev'\n",
    "RESULT_FILE_PREFIX = RESULT_DIR +'/module_11_01.'\n",
    "STEP01_DATA_UNDERSTANDING = 'step01.data_understanding.'\n",
    "STEP02_DATA_PREPARATION = 'step02.data_preparation.'\n",
    "STEP03_MODELING = 'step03.modeling.'\n",
    "STEP04_EVALUATION = 'step04.evaluation.'\n",
    "\n",
    "TARGET_FIELD = 'price'\n",
    "ALPHA_START = 0.00001\n",
    "ALPHA_END = 100\n",
    "ALPHA_STEP = (ALPHA_END - ALPHA_START)/10\n",
    "ALPHA_RANGE = np.arange(ALPHA_START, ALPHA_END, ALPHA_STEP)\n",
    "IGNORE_FEATURES = ['VIN', 'id', 'model', 'paint_color']\n",
    "CONFIG_NODE_KEY = 'config'\n",
    "Q3_PERCENT_DEFAULT = 75\n",
    "Q1_PERCENT_DEFAULT = 25\n",
    "\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.makedirs(RESULT_DIR)\n",
    "\n",
    "##############################\n",
    "# Common global methods\n",
    "##############################\n",
    "def writeString2File(string2Write, path, print2Screen = True):\n",
    "    if print2Screen:\n",
    "        print(string2Write)\n",
    "    with open(path, \"w\") as text_file:\n",
    "        text_file.write(str(string2Write))\n",
    "\n",
    "def dataFrame2Image(inputDf, path, baseHeight=480, height_per_row=20, char_limit=30, height_padding=16.5, baseWidth=1200):\n",
    "    total_height = 0 + baseHeight\n",
    "    for x in range(inputDf.shape[0]):\n",
    "        total_height += height_per_row\n",
    "    for y in range(inputDf.shape[1]):\n",
    "        if len(str(inputDf.iloc[x][y])) > char_limit:\n",
    "            total_height += height_padding\n",
    "\n",
    "    height = total_height\n",
    "    width = baseWidth\n",
    "    layout = {\n",
    "        'height': height, \n",
    "        'width': width,\n",
    "        'margin': {'r': 1, 'l': 1, 't': 1, 'b': 1}\n",
    "    }\n",
    "    \n",
    "    fig = go.Figure(data=[\n",
    "            go.Table(\n",
    "                header=dict(values=list(inputDf.columns), fill_color='paleturquoise', align='left'),\n",
    "                cells=dict(values=inputDf.transpose().values.tolist(), fill_color='lavender', align='left')\n",
    "            )], layout=layout)\n",
    "    fig.show()\n",
    "    \n",
    "    pio.write_image(fig, path, scale=1, width=width, height=total_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Business Understanding:</b> Determine which combination of numeric and categorical features and corresponding categorical values most contribute to driving price increase for used cars. Rank the features and values based on weight to give the used car dealers a simple way to prioritize used cars to display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rawDf = pd.read_csv('./data/vehicles.csv')\n",
    "\n",
    "def writeDataFrame2Excel(inputDf, name):\n",
    "    with pd.ExcelWriter(RESULT_FILE_PREFIX + name + 'data.frame.xlsx') as writer:\n",
    "        inputDf.to_excel(writer)\n",
    "    \n",
    "def writeDataFrameDetails(data, fileNameSuffix, titleSuffix='', width=480, height=400, bins=100):\n",
    "    buffer = io.StringIO()\n",
    "    data.info(verbose=True, buf=buffer, show_counts=True)\n",
    "    writeString2File(buffer.getvalue(), RESULT_FILE_PREFIX + fileNameSuffix + 'data.info.txt')\n",
    "    writeString2File(data.describe(), RESULT_FILE_PREFIX + fileNameSuffix + 'data.describe.txt')\n",
    "    \n",
    "    plotDistribution4Fields = data.select_dtypes(include=[np.number]).columns.values\n",
    "    graphsPerRow = len(plotDistribution4Fields)\n",
    "    if graphsPerRow > 0:\n",
    "        widthInches = width * graphsPerRow/100\n",
    "        heightInches = height * 2/100\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(2, len(plotDistribution4Fields), figsize=(widthInches, heightInches))\n",
    "        subPlotRow = 0\n",
    "        subPlotCol = 0\n",
    "        cm = sns.color_palette(\"plasma\",bins)\n",
    "        for field in plotDistribution4Fields:\n",
    "            axSub1 = sns.boxplot(ax=ax[0,subPlotCol], data=data[[field]], palette='pastel')\n",
    "            axSub2 = sns.histplot(ax=ax[1,subPlotCol], data=data, x=field, bins=bins)\n",
    "            for bin_,i in zip(axSub2.patches,cm):\n",
    "                bin_.set_facecolor(i)\n",
    "            subPlotCol += 1\n",
    "\n",
    "        fig.suptitle('Data Distribution' + titleSuffix)\n",
    "        plt.savefig(RESULT_FILE_PREFIX + fileNameSuffix + 'data.distribution.png')\n",
    "        \n",
    "    \n",
    "\n",
    "# Reduce data set \n",
    "if DEV_MODE == True:\n",
    "    rawDf  = rawDf.dropna()\n",
    "    rawDf = rawDf.head(10000)\n",
    "\n",
    "writeDataFrameDetails(data=rawDf, fileNameSuffix=STEP01_DATA_UNDERSTANDING, titleSuffix=': Raw Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixedCounter = {}\n",
    "def showBarPlot(request, data, xColumn, yColumn, colorColumn, path, xLogScale=False, yLogScale=False, width=1200, height=600, dataLabelRotation=0, moveLegendBotttom=False):\n",
    "    labels = request['labels']\n",
    "    xAxesLabel = labels[xColumn]\n",
    "    if xLogScale:\n",
    "        xAxesLabel += ' (Log Scale)'\n",
    "    yAxesLabel = labels[yColumn]\n",
    "    if yLogScale:\n",
    "        yAxesLabel += ' (Log Scale)'\n",
    "    title = yAxesLabel + ' vs ' + xAxesLabel\n",
    "    barWidthPixels = 80\n",
    "    if data.shape[0] > 0:\n",
    "        palette='pastel'\n",
    "        barWidthInches = barWidthPixels / 100\n",
    "        # Width of graph is number of rows * bar_width \n",
    "        widthInches = math.ceil(width / 100)\n",
    "        heightInches = math.ceil(height / 100)\n",
    "        legendLocation='lower left'\n",
    "        bboxAnchor=(0, -0.5)\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        axSub = sns.barplot(data=data, x=xColumn, y=yColumn, hue=colorColumn, \n",
    "                         palette=palette, width=barWidthInches)\n",
    "        axSub.set_title(title)\n",
    "        axSub.set(xlabel=xAxesLabel, ylabel=yAxesLabel)\n",
    "        for bars_group in axSub.containers: \n",
    "            axSub.bar_label(bars_group, padding=3, fontsize=8, rotation=dataLabelRotation, label_type='center')\n",
    "        if yLogScale:\n",
    "            plt.yscale('log')\n",
    "        if xLogScale:\n",
    "            plt.xscale('log')\n",
    "        if moveLegendBotttom:\n",
    "            sns.move_legend(axSub, legendLocation, bbox_to_anchor=bboxAnchor)\n",
    "        axSub.figure.set_size_inches(widthInches, heightInches)\n",
    "        \n",
    "        plt.savefig(path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "def showDataPreparationBarPlot(request, data, xColumn, yColumn, colorColumn, fileSuffix, xLogScale=False, yLogScale=False, width=1200, height=600):\n",
    "    filePrefix = RESULT_FILE_PREFIX + STEP02_DATA_PREPARATION\n",
    "    showBarPlot(request=request, data=data, xColumn=xColumn, yColumn=yColumn, colorColumn=colorColumn, \n",
    "                path=filePrefix + fileSuffix, xLogScale=xLogScale, yLogScale=yLogScale, width=width, height=height, dataLabelRotation=90, moveLegendBotttom=True)\n",
    "\n",
    "def displayStats(request, dataReportDf, restrictToStages = []):\n",
    "    suffix = ''\n",
    "    if(len(restrictToStages) > 0):\n",
    "        suffix += '-'.join(restrictToStages) + '.'\n",
    "    labels = request['labels']\n",
    "    filteredDf = dataReportDf\n",
    "    if len(restrictToStages) > 0:\n",
    "        filteredDf = dataReportDf.loc[dataReportDf['stage'].isin(restrictToStages)]\n",
    "\n",
    "    columnNames = list(filteredDf['column'].value_counts().index)\n",
    "    if 'excludeColumnsFromReport' in request:\n",
    "        excludeColumns = request['excludeColumnsFromReport']\n",
    "        for k in excludeColumns:\n",
    "            if k in columnNames:\n",
    "                columnNames.remove(k)\n",
    "    filteredDf = filteredDf.loc[filteredDf['column'].isin(columnNames)]\n",
    "    \n",
    "    xColumn = 'column'\n",
    "    colorColumn = 'stage'\n",
    "    \n",
    "    yColumn = 'row_count'\n",
    "    data = filteredDf.loc[filteredDf['column'] == columnNames[0]]\n",
    "    showDataPreparationBarPlot(request=request, data=data, xColumn=xColumn, yColumn=yColumn, colorColumn=colorColumn, \n",
    "                               fileSuffix=suffix + 'row_count.png', height=800)\n",
    "    \n",
    "    yColumn = 'na_value_count'\n",
    "    data = filteredDf[filteredDf[yColumn] != 0]\n",
    "    showDataPreparationBarPlot(request=request, data=data, xColumn=xColumn, yColumn=yColumn, colorColumn=colorColumn, \n",
    "                               fileSuffix=suffix + 'missing_count.png', yLogScale=True, height=800, width=2400)\n",
    "\n",
    "    yColumn = 'na_value_pct'\n",
    "    data = filteredDf[filteredDf[yColumn] != 0]\n",
    "    showDataPreparationBarPlot(request=request, data=data, xColumn=xColumn, yColumn=yColumn, colorColumn=colorColumn, \n",
    "                               fileSuffix=suffix + 'missing_percentage.png', yLogScale=True, height=800, width=2400)\n",
    "\n",
    "    yColumn = 'unq_value_count'\n",
    "    data = filteredDf.loc[filteredDf['data_type'] == 'object']\n",
    "    showDataPreparationBarPlot(request=request, data=data, xColumn=xColumn, yColumn=yColumn, colorColumn=colorColumn, \n",
    "                               fileSuffix=suffix + 'unique_values.png', yLogScale=True, height=800, width=2400)\n",
    "\n",
    "def getMostCommonValues(inputDf, groupByFields, columns):\n",
    "    finalList = groupByFields\n",
    "    cleanedDf = inputDf.dropna(subset = finalList)\n",
    "    responseDf = cleanedDf.groupby(groupByFields)[columns].agg(pd.Series.mode).reset_index()\n",
    "    return responseDf\n",
    "    \n",
    "def getType(row, orginalDf):\n",
    "    return str(orginalDf.dtypes[row['column']])\n",
    "    \n",
    "def getUniqueCounts(row, orginalDf):\n",
    "    return len(orginalDf[row['column']].unique())\n",
    "\n",
    "def fillEmptyValues(row, lookupTable, groupByFields, fillFields):\n",
    "    global fixedCounter\n",
    "    key = ''\n",
    "    for gf in groupByFields:\n",
    "        key += str(row[gf])\n",
    "    if key in lookupTable:\n",
    "        match = lookupTable[key]\n",
    "        for ff in fillFields:\n",
    "            if pd.isna(row[ff]):\n",
    "                if ff not in fixedCounter:\n",
    "                    fixedCounter[ff] = 0\n",
    "                fixedCounter[ff] += 1\n",
    "                row[ff] = match[ff]\n",
    "    return row\n",
    "\n",
    "def findAndFixMissingUsingMode(inputDf, groupByFields, fillFields):\n",
    "    lookupTable = getMostCommonValues(inputDf, groupByFields, fillFields)\n",
    "    cache = {}\n",
    "    for index, row in lookupTable.iterrows():\n",
    "        key = ''\n",
    "        for gf in groupByFields:\n",
    "            key += str(row[gf])\n",
    "        node = {}\n",
    "        cache[key] = node\n",
    "        for f in fillFields:\n",
    "            node[f] = None\n",
    "            val = row[f]\n",
    "            if isinstance(val,np.ndarray):\n",
    "                if len(val) > 0:\n",
    "                    val = val[0]\n",
    "            else:\n",
    "                node[f] = val\n",
    "    finalDf = inputDf.apply(fillEmptyValues, lookupTable = cache, groupByFields = groupByFields, fillFields = fillFields, axis = 1)\n",
    "    return finalDf\n",
    "\n",
    "def addMetadataStatistics(stage, inputDf, \n",
    "                          dataReportDf = pd.DataFrame(columns = [\n",
    "                              'stage', \n",
    "                              'row_count', \n",
    "                              'column', \n",
    "                              'data_type', \n",
    "                              'na_value_count', \n",
    "                              'na_value_pct', \n",
    "                              'unq_value_count'])):\n",
    "    naSeries = inputDf.isna().sum()\n",
    "    numRows = inputDf.shape[0]\n",
    "    \n",
    "    metaDf = pd.DataFrame()\n",
    "    metaDf['column'] = naSeries.index\n",
    "    metaDf['data_type'] = metaDf.apply(getType, orginalDf=inputDf, axis=1)\n",
    "    metaDf['na_value_count']= naSeries.values\n",
    "    metaDf['na_value_pct']= metaDf['na_value_count'] * 100 / numRows\n",
    "    metaDf['unq_value_count'] = metaDf.apply(getUniqueCounts, orginalDf=inputDf, axis=1)\n",
    "    metaDf['stage'] = stage\n",
    "    metaDf['row_count'] = numRows\n",
    "\n",
    "    for index, row in metaDf.iterrows():\n",
    "        dataReportDf.loc[dataReportDf.shape[0]] = row\n",
    "    return dataReportDf\n",
    "\n",
    "def doDrop(operationNode, inputDf, dataReportDf):\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    keys = []\n",
    "    for k in configNode:\n",
    "        keys.append(k)\n",
    "        inputDf = inputDf.drop(k, axis=1)\n",
    "    return inputDf, dataReportDf\n",
    "\n",
    "def doDropNa(operationNode, inputDf, dataReportDf):\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    keys = []\n",
    "    for k in configNode:\n",
    "        keys.append(k)\n",
    "        inputDf = inputDf.dropna(axis=0, subset=[k])\n",
    "    dataReportDf = addMetadataStatistics(f'dropna[{keys}]', inputDf, dataReportDf)\n",
    "    return inputDf, dataReportDf\n",
    "    \n",
    "def doFillNa(operationNode, inputDf, dataReportDf):\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    for k in configNode:\n",
    "        node = configNode[k]\n",
    "        value2Fill = node['value']\n",
    "        inputDf[k].fillna(value2Fill, inplace=True)\n",
    "    return inputDf, dataReportDf\n",
    "\n",
    "def doFillUsingMode(operationNode, inputDf, dataReportDf):\n",
    "    outputDf = inputDf\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    for k in configNode:\n",
    "        node = configNode[k]\n",
    "        lookupFields = node['lookupFields']\n",
    "        fillFields = node['fillFields']\n",
    "        outputDf = findAndFixMissingUsingMode(outputDf, lookupFields, fillFields)\n",
    "        dataReportDf = addMetadataStatistics(f'fill{fillFields} = lookup{lookupFields}', outputDf, dataReportDf)\n",
    "    return outputDf, dataReportDf\n",
    "\n",
    "def doQueryFiltering(operationNode, inputDf, dataReportDf):\n",
    "    outputDf = inputDf\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    for k in configNode:\n",
    "        node = configNode[k]\n",
    "        query = node['query']\n",
    "        outputDf = outputDf.query(query)\n",
    "        dataReportDf = addMetadataStatistics(f'query({k})', outputDf, dataReportDf)\n",
    "    return outputDf, dataReportDf\n",
    "    \n",
    "def doIqrFiltering(operationNode, inputDf, dataReportDf):\n",
    "    outputDf = inputDf\n",
    "    # NOTE: have to calculate quartiles all at once otherwise quartiles shift as we cull the dataset\n",
    "    quartileRanges = {}\n",
    "    outputDf = inputDf\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    \n",
    "    for k in configNode:\n",
    "        node = configNode[k]\n",
    "        q3Threshold = 0.75\n",
    "        q1Threshold = 0.25\n",
    "        if 'q3%' in node:\n",
    "            q3Threshold = float(node['q3%']) / 100\n",
    "        if 'q1%' in node:\n",
    "            q1Threshold = float(node['q1%']) / 100\n",
    "        Q3 = np.quantile(outputDf[k], q3Threshold)\n",
    "        Q1 = np.quantile(outputDf[k], q1Threshold)\n",
    "        IQR = Q3 - Q1\n",
    "        lowerRange = Q1 - 1.5 * IQR\n",
    "        upperRange = Q3 + 1.5 * IQR\n",
    "        quartileRanges[k]= {\n",
    "            'q3%': q3Threshold * 100,\n",
    "            'q1%': q1Threshold * 100,\n",
    "            'lowerRange': lowerRange,\n",
    "            'upperRange': upperRange\n",
    "        }\n",
    "\n",
    "    # Now do all quartiles together\n",
    "    for k in quartileRanges:\n",
    "        node = quartileRanges[k]\n",
    "        q3 = node['q3%']\n",
    "        q1 = node['q1%']\n",
    "        lowerRange = node['lowerRange']\n",
    "        upperRange = node['upperRange']\n",
    "        iqrStageDisplayName = f'iqrFilter({k}, {lowerRange} [{q1}%], {upperRange}[{q3}%])' \n",
    "        outputDf = outputDf.loc[(outputDf[k] >= lowerRange) & (outputDf[k] <= upperRange)]\n",
    "        dataReportDf = addMetadataStatistics(iqrStageDisplayName, outputDf, dataReportDf)\n",
    "    return outputDf, dataReportDf\n",
    "\n",
    "def doToLowerCase(operationNode, inputDf, dataReportDf):\n",
    "    configNode = operationNode[CONFIG_NODE_KEY]\n",
    "    for k in configNode:\n",
    "        node = configNode[k]\n",
    "        targetField = k;\n",
    "        if 'field' in node:\n",
    "            targetField = node['field']\n",
    "        inputDf[targetField] = inputDf[k].str.lower()\n",
    "        if 'removeSpaces' in node:\n",
    "            if node['removeSpaces'] == True:\n",
    "                inputDf[targetField] = inputDf[targetField].str.replace(\" \", \"\")\n",
    "    return inputDf, dataReportDf\n",
    "\n",
    "def preProcessData(request, inputDf, dataReportDf):\n",
    "    # Copy once then use same for all transforms\n",
    "    responseDf = inputDf.copy()\n",
    "    operations = request['operations']\n",
    "    for node in operations:\n",
    "        operationName = node['operation']\n",
    "        print(f'Processing: {operationName}')\n",
    "        if operationName == 'drop':\n",
    "            responseDf, dataReportDf = doDrop(node, responseDf, dataReportDf)\n",
    "        elif operationName == 'dropna':\n",
    "            responseDf, dataReportDf = doDropNa(node, responseDf, dataReportDf)\n",
    "        elif operationName == 'fillna':\n",
    "            responseDf, dataReportDf = doFillNa(node, responseDf, dataReportDf)\n",
    "        elif operationName == 'queryFilter':\n",
    "            responseDf, dataReportDf = doQueryFiltering(node, responseDf, dataReportDf)\n",
    "        elif operationName == 'iqr':\n",
    "            responseDf, dataReportDf = doIqrFiltering(node, responseDf, dataReportDf)\n",
    "        elif operationName == 'toLowerCase':\n",
    "            responseDf, dataReportDf = doToLowerCase(node, responseDf, dataReportDf)\n",
    "        elif operationName == 'fillUsingMode':\n",
    "            responseDf, dataReportDf = doFillUsingMode(node, responseDf, dataReportDf)\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported operation: {operation}\")\n",
    "    \n",
    "    return responseDf, dataReportDf\n",
    "\n",
    "\n",
    "dataPreparationRequest = {\n",
    "    'excludeColumnsFromReport': ['VIN', 'id', 'cleaned_model'],\n",
    "    'operations': [\n",
    "         {\n",
    "                'operation': 'drop',\n",
    "                'config' : {\n",
    "                    'VIN': {},\n",
    "                    'id': {},\n",
    "                    'paint_color': {},\n",
    "                }\n",
    "         },\n",
    "         {\n",
    "            'operation': 'toLowerCase',\n",
    "            'config' : {\n",
    "                'state': {},\n",
    "                'region': {},\n",
    "                'condition': {},\n",
    "                'cylinders': {},\n",
    "                'fuel': {},\n",
    "                'title_status': {},\n",
    "                'transmission': {},\n",
    "                'drive': {},\n",
    "                'manufacturer': {},\n",
    "                'size': {},\n",
    "                'type': {},\n",
    "                'model': {\n",
    "                    'field': 'cleaned_model',\n",
    "                    'removeSpaces': True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'operation': 'dropna',\n",
    "            'config': {\n",
    "                'price': {},\n",
    "                'model': {}, \n",
    "                'manufacturer': {},\n",
    "                'odometer': {},\n",
    "                'year': {}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'operation': 'iqr',\n",
    "            'config': {\n",
    "                'price': {\n",
    "                    'q3%': Q3_PERCENT_DEFAULT,\n",
    "                    'q1%': Q1_PERCENT_DEFAULT\n",
    "                },\n",
    "                'odometer': {\n",
    "                    'q3%': Q3_PERCENT_DEFAULT,\n",
    "                    'q1%': Q1_PERCENT_DEFAULT\n",
    "                },\n",
    "                'year': {\n",
    "                    'q3%': Q3_PERCENT_DEFAULT,\n",
    "                    'q1%': Q1_PERCENT_DEFAULT\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'operation': 'queryFilter',\n",
    "            'config': {\n",
    "                'price > 0': {\n",
    "                    'query': 'price > 0'\n",
    "                },\n",
    "                'odometer > 0': {\n",
    "                    'query': 'odometer > 0'\n",
    "                },\n",
    "                'year > 1900': {\n",
    "                    'query': 'year > 1900'\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'operation': 'fillna',\n",
    "            'config': {\n",
    "                'condition': {\n",
    "                    'value': 'unknown'\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'operation': 'fillUsingMode',\n",
    "            'config': {\n",
    "                'Lookup&Fill Pass1': {\n",
    "                    'lookupFields': ['manufacturer', 'cleaned_model', 'year'],\n",
    "                    'fillFields': ['cylinders', 'size', 'type', 'drive']\n",
    "                },\n",
    "                'Lookup&Fill Pass2': {\n",
    "                    'lookupFields': ['manufacturer', 'cleaned_model'],\n",
    "                    'fillFields': ['cylinders', 'size', 'type', 'drive']\n",
    "                },\n",
    "                'Lookup&Fill Pass3': {\n",
    "                    'lookupFields': ['manufacturer', 'type'],\n",
    "                    'fillFields': ['size']\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'operation': 'fillna',\n",
    "            'config': {\n",
    "                'cylinders': {\n",
    "                    'value': 'unknown'\n",
    "                },\n",
    "                'title_status': {\n",
    "                    'value': 'unknown'\n",
    "                },\n",
    "                'transmission': {\n",
    "                    'value': 'unknown'\n",
    "                },\n",
    "                'drive': {\n",
    "                    'value': 'unknown'\n",
    "                },\n",
    "                'fuel': {\n",
    "                    'value': 'unknown'\n",
    "                },\n",
    "                'size': {\n",
    "                    'value': 'unknown'\n",
    "                },\n",
    "                'type': {\n",
    "                    'value': 'unknown'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    'labels': {\n",
    "        'row_count': 'Row Count',\n",
    "        'na_value_count': 'Empty Value #',\n",
    "        'na_value_pct': 'Empty Value %',\n",
    "        'unq_value_count': 'Unique Value Counts',\n",
    "        'column': 'Feature'\n",
    "    }\n",
    "}\n",
    "dataReportDf = addMetadataStatistics('raw', rawDf)\n",
    "preProcessedDf, dataReportDf = preProcessData(dataPreparationRequest, rawDf, dataReportDf)\n",
    "writeDataFrameDetails(data=preProcessedDf, fileNameSuffix=STEP02_DATA_PREPARATION, titleSuffix=': Data Cleaning & Preparation')\n",
    "displayStats(dataPreparationRequest, dataReportDf)\n",
    "writeString2File(json.dumps(dataPreparationRequest, indent=4), RESULT_FILE_PREFIX + STEP02_DATA_PREPARATION + '.request.json')\n",
    "writeDataFrame2Excel(dataReportDf, STEP02_DATA_PREPARATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encodeAndAddCategoricalColumns(inputDf, targetField, categoricalFeatures):\n",
    "    lowCardinalityFeatures = []\n",
    "    highCardinalityFeatures = []\n",
    "\n",
    "    renamedFields = {}\n",
    "    for cf in categoricalFeatures:\n",
    "        if cf != targetField:\n",
    "            unqValues = len(inputDf[cf].value_counts())\n",
    "            if unqValues > 255:\n",
    "                renamedFields[cf] = 'encoded_' + cf\n",
    "                highCardinalityFeatures.append(cf)\n",
    "            else:\n",
    "                lowCardinalityFeatures.append(cf)\n",
    "            \n",
    "    transformerSet = []\n",
    "    if(len(lowCardinalityFeatures) > 0):\n",
    "        transformerSet.append(('categorical_low_cardinality', OneHotEncoder(drop=\"if_binary\", sparse_output=False), lowCardinalityFeatures))\n",
    "\n",
    "    enhancedDf = inputDf\n",
    "    \n",
    "    # https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69\n",
    "    if(len(highCardinalityFeatures) > 0):\n",
    "        if USE_TARGET_ENCODER_FOR_HIGH_CARDINALTY:\n",
    "            transformerSet.append(('categorical_high_cardinality', TargetEncoder(), highCardinalityFeatures))\n",
    "        else:\n",
    "            labelEncoder = LabelEncoder()\n",
    "            for hcf in highCardinalityFeatures:\n",
    "                enhancedDf['encoded_' + hcf] = labelEncoder.fit_transform(enhancedDf[hcf])\n",
    "\n",
    "    \n",
    "        \n",
    "    columnTransformer = ColumnTransformer(transformerSet, verbose_feature_names_out=False)\n",
    "    columnTransformer.set_output(transform='pandas')\n",
    "    \n",
    "\n",
    "    \n",
    "    encodedDf = columnTransformer.fit_transform(enhancedDf, enhancedDf[targetField])\n",
    "    encodedFeatureNames = columnTransformer.get_feature_names_out()\n",
    "    encodedDf = encodedDf[encodedFeatureNames]\n",
    "    if USE_TARGET_ENCODER_FOR_HIGH_CARDINALTY:\n",
    "        encodedDf.rename(columns=renamedFields, inplace=True)\n",
    "    enhancedDf = enhancedDf.join(encodedDf)\n",
    "    return enhancedDf\n",
    "\n",
    "def getRequestTemplate(targetField, numericFeaturesUsed, categoricalFeaturesUsed):\n",
    "    requestTemplate = {\n",
    "        'numericFeatures': numericFeaturesUsed,\n",
    "        'categoricalFeatures': categoricalFeaturesUsed,\n",
    "        'targetField': targetField,\n",
    "        'featureTracker': {\n",
    "            'features': {}\n",
    "        }\n",
    "    }\n",
    "    return requestTemplate\n",
    "    \n",
    "def createRequestFromTemplate(requestTemplate, experimentName, estimator , parametersToTry = {}, extractImportantFeaturesFn = None):\n",
    "    request = copy.deepcopy(requestTemplate)\n",
    "    request['experimentName'] = experimentName\n",
    "    request['estimator'] = estimator\n",
    "    request['parametersToTry'] = parametersToTry\n",
    "    request['extractImportantFeaturesFn'] = extractImportantFeaturesFn\n",
    "    return request\n",
    "\n",
    "numericFeatures = list(preProcessedDf.select_dtypes(include=[np.number]).columns.values)\n",
    "categoricalFeatures = list(preProcessedDf.select_dtypes(exclude=[np.number]).columns.values)\n",
    "print('Original Numeric Features: '+ str(numericFeatures))\n",
    "print('Original Categorical Features: '+ str(categoricalFeatures))\n",
    "\n",
    "####################\n",
    "# Setup\n",
    "####################\n",
    "# Leave out \n",
    "# - VIN - No usuable information in this and no decisions based on vin based on domain knowledge\n",
    "# - id - No usuable information in this and no decisions based on vin based on domain knowledge\n",
    "# - paint_color - Lots of missing values that we cannot impute and reflects local preferences/tastes\n",
    "# - model, cleaned_model - Too many values model does not return in reasonable time\n",
    "\n",
    "\n",
    "numericFeaturesUsed = numericFeatures.copy()\n",
    "categoricalFeaturesUsed = categoricalFeatures.copy()\n",
    "for rf in IGNORE_FEATURES:\n",
    "    if rf in numericFeaturesUsed:\n",
    "        numericFeaturesUsed.remove(rf)\n",
    "    if rf in categoricalFeaturesUsed:\n",
    "        categoricalFeaturesUsed.remove(rf)\n",
    "\n",
    "# Do transform for categorical now because baking into pipeline means sometings columns in training set aren't in test and vice versa\n",
    "experimentDf = encodeAndAddCategoricalColumns(preProcessedDf, TARGET_FIELD, categoricalFeaturesUsed)\n",
    "\n",
    "# Need to pull out any numeric fields like id that made it through\n",
    "numericFeatures2Try = list(experimentDf.select_dtypes(include=[np.number]).columns.values)\n",
    "for rf in IGNORE_FEATURES:\n",
    "    if rf in numericFeatures2Try:\n",
    "        numericFeatures2Try.remove(rf)\n",
    "\n",
    "def drawCorrelation(data, name, minValueAbs = 0, drawPairPlot = False):\n",
    "    corr = data.corr(numeric_only=True)\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    corr = corr.mask(mask | ((corr > -1 * minValueAbs) & (corr <= minValueAbs)), None)    \n",
    "    corr.dropna(axis = 0, how = 'all', inplace = True)\n",
    "    corr.dropna(axis = 1, how = 'all', inplace = True)\n",
    "    height= max(corr.shape[0] * 100, 640) / 100\n",
    "    width = max(corr.shape[1] * 100, 640) / 100\n",
    "    sns.set (rc = {'figure.figsize':(width, height)})\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "\n",
    "    writeDataFrame2Excel(corr, STEP02_DATA_PREPARATION + name + '.correlation.');\n",
    "    \n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, cmap=cmap, annot=True, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    title = f'Correlation Matrix for {name}'\n",
    "    if minValueAbs != 0:\n",
    "        title += f' (x <= -{minValueAbs} OR x >= {minValueAbs})'\n",
    "    plt.title(title)\n",
    "    plt.savefig(RESULT_FILE_PREFIX + STEP02_DATA_PREPARATION + name + '.data.heatmap.png')\n",
    "    plt.show()\n",
    "\n",
    "    # NOTE: Be careful very slow for large sets\n",
    "    if True == drawPairPlot:\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        sns.pairplot(data)\n",
    "        plt.savefig(RESULT_FILE_PREFIX + STEP02_DATA_PREPARATION + name + '.data.pairplot.png')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "drawCorrelation(data = rawDf, name = 'raw')\n",
    "drawCorrelation(data = preProcessedDf, name = 'preProcessed')\n",
    "drawCorrelation(data = experimentDf, name = 'final', minValueAbs = 0.2)\n",
    "print('Final Numeric Features: '+ str(numericFeatures2Try))\n",
    "experimentDf = experimentDf[numericFeatures2Try]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runFeatureSelectionAnalysis(request, inputDf, modelReportDf, featureImportanceReportDf):\n",
    "    experimentName = request['experimentName']\n",
    "    numericFeatures = request['numericFeatures']\n",
    "    categoricalFeatures = request['categoricalFeatures']\n",
    "    targetField = request['targetField']\n",
    "    estimator = request['estimator']\n",
    "    parametersToTry = request['parametersToTry']\n",
    "    featureTracker = request['featureTracker']\n",
    "    extractImportantFeaturesFn = request['extractImportantFeaturesFn']\n",
    "\n",
    "    # Make copy of data\n",
    "    experimentDf = inputDf\n",
    "\n",
    "    # Just in case remove target field from feeatures\n",
    "    if targetField in numericFeatures:\n",
    "        numericFeatures.remove(targetField)\n",
    "        \n",
    "    # Build train + dev set\n",
    "    X = experimentDf.drop(targetField, axis=1);\n",
    "    y = experimentDf[targetField]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=42)\n",
    "\n",
    "    gridSearchCV = GridSearchCV(estimator = estimator, \n",
    "                                        param_grid = parametersToTry, \n",
    "                                        scoring = \"neg_mean_squared_error\",\n",
    "                                        cv = 5)\n",
    "    gridSearchCV.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best performer\n",
    "    bestEstimator = gridSearchCV.best_estimator_\n",
    "    bestScore = gridSearchCV.best_score_\n",
    "    bestParameters = json.dumps(gridSearchCV.best_params_, indent=2)\n",
    "    #print(\"CV Results\" + str(gridSearchCV.cv_results_))\n",
    "    \n",
    "    \n",
    "    # Display processing pipeline\n",
    "    display(gridSearchCV)\n",
    "\n",
    "    # Build cofficient importances\n",
    "    coefDf = extractImportantFeaturesFn(bestEstimator)\n",
    "    coefDf['Model'] = experimentName\n",
    "    featureImportanceReportDf = pd.concat([featureImportanceReportDf, coefDf])\n",
    "\n",
    "    # Get predictions for train and test sets\n",
    "    y_predict_train = bestEstimator.predict(X_train)\n",
    "    y_predict_test = bestEstimator.predict(X_test)\n",
    "    \n",
    "    # Add to report\n",
    "    modelReportDf.loc[len(modelReportDf)] = [\n",
    "        experimentName, \n",
    "        ', '.join(categoricalFeatures), \n",
    "        ', '.join(numericFeatures),\n",
    "        bestParameters,\n",
    "        round(metrics.mean_squared_error(y_predict_train, y_train), 2), \n",
    "        round(metrics.mean_squared_error(y_predict_test, y_test), 2), \n",
    "        round(metrics.mean_absolute_error(y_predict_train, y_train), 2), \n",
    "        round(metrics.mean_absolute_error(y_predict_test, y_test), 2), \n",
    "        round(metrics.r2_score(y_predict_train, y_train), 2), \n",
    "        round(metrics.r2_score(y_predict_test, y_test), 2) \n",
    "    ]\n",
    "\n",
    "    # Track feature performance across runs\n",
    "    for row in coefDf.itertuples():\n",
    "        field = row.index\n",
    "        if field not in featureTracker:\n",
    "            featureTracker[field] = {\n",
    "                'count': 0,\n",
    "                'score': 0.0\n",
    "            }\n",
    "        featureTracker[field]['count'] = featureTracker[field]['count'] + 1\n",
    "    \n",
    "    return modelReportDf, featureImportanceReportDf\n",
    "    \n",
    "\n",
    "# Setup base request teamplate\n",
    "REQUEST_TEMPLATE = getRequestTemplate(TARGET_FIELD, numericFeaturesUsed, categoricalFeaturesUsed)\n",
    "\n",
    "# Setup processing reporting dataframes\n",
    "modelReportDf = pd.DataFrame(columns=[\n",
    "    'Model', \n",
    "    'Categorical Features', \n",
    "    'Numerical Features', \n",
    "    'Best Parameters', \n",
    "    'Train MSE', \n",
    "    'Test MSE',\n",
    "    'Train MAE', \n",
    "    'Test MAE',\n",
    "    'Train R2', \n",
    "    'Test R2'\n",
    "])\n",
    "featureImportanceReportDf = pd.DataFrame(columns=['Model'])\n",
    "\n",
    "####################\n",
    "# Model 1: Ridge\n",
    "####################\n",
    "if MODEL1_ACTIVE:\n",
    "    pipeline1 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge())\n",
    "    ], )\n",
    "    parameters1 = {\n",
    "        'regressor__alpha' : ALPHA_RANGE\n",
    "    }\n",
    "    def getImportantFeatures1(bestEstimator):\n",
    "        feature_names = bestEstimator[:-1].get_feature_names_out()\n",
    "        bestFeatureCoefficients = bestEstimator[-1].coef_\n",
    "        coefDf = pd.DataFrame(\n",
    "            bestFeatureCoefficients,\n",
    "            columns=[\"coefficient\"],\n",
    "            index=feature_names\n",
    "        ).reset_index()\n",
    "        return coefDf\n",
    "    \n",
    "    request1 = createRequestFromTemplate(REQUEST_TEMPLATE, 'Ridge', pipeline1, parameters1, getImportantFeatures1)\n",
    "    modelReportDf, featureImportanceReportDf = runFeatureSelectionAnalysis(request1, experimentDf, modelReportDf, featureImportanceReportDf)\n",
    "\n",
    "####################\n",
    "# Model 2: Lasso\n",
    "####################\n",
    "if MODEL2_ACTIVE:\n",
    "    pipeline2 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso())\n",
    "    ], )\n",
    "    parameters2 = {\n",
    "        'regressor__alpha' : ALPHA_RANGE\n",
    "    }\n",
    "    def getImportantFeatures2(bestEstimator):\n",
    "        feature_names = bestEstimator[:-1].get_feature_names_out()\n",
    "        bestFeatureCoefficients = bestEstimator[-1].coef_\n",
    "        coefDf = pd.DataFrame(\n",
    "            bestFeatureCoefficients,\n",
    "            columns=[\"coefficient\"],\n",
    "            index=feature_names\n",
    "        ).reset_index()\n",
    "        return coefDf\n",
    "    \n",
    "    request2 = createRequestFromTemplate(REQUEST_TEMPLATE, 'Lasso', pipeline2, parameters2, getImportantFeatures2)\n",
    "    modelReportDf, featureImportanceReportDf = runFeatureSelectionAnalysis(request2, experimentDf, modelReportDf, featureImportanceReportDf)\n",
    "\n",
    "####################\n",
    "# Model 3: ElasticNet\n",
    "####################\n",
    "if MODEL3_ACTIVE:\n",
    "    pipeline3 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', ElasticNet())\n",
    "    ], )\n",
    "    parameters3 = {\n",
    "        'regressor__alpha' : ALPHA_RANGE\n",
    "    }\n",
    "    def getImportantFeatures3(bestEstimator):\n",
    "        feature_names = bestEstimator[:-1].get_feature_names_out()\n",
    "        bestFeatureCoefficients = bestEstimator[-1].coef_\n",
    "        coefDf = pd.DataFrame(\n",
    "            bestFeatureCoefficients,\n",
    "            columns=[\"coefficient\"],\n",
    "            index=feature_names\n",
    "        ).reset_index()\n",
    "        return coefDf\n",
    "    \n",
    "    request3 = createRequestFromTemplate(REQUEST_TEMPLATE, 'ElasticNet', pipeline3, parameters3, getImportantFeatures3)\n",
    "    modelReportDf, featureImportanceReportDf = runFeatureSelectionAnalysis(request3, experimentDf, modelReportDf, featureImportanceReportDf)\n",
    "\n",
    "####################\n",
    "# Model 4: RandomForestRegressor\n",
    "####################\n",
    "if MODEL4_ACTIVE:\n",
    "    pipeline4 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ], )\n",
    "    parameters4 = {\n",
    "        'regressor__n_estimators': [100, 200, 300],\n",
    "        'regressor__max_depth': [None, 1, 10, 20]\n",
    "    }\n",
    "    def getImportantFeatures4(bestEstimator):\n",
    "        feature_names = bestEstimator[:-1].get_feature_names_out()\n",
    "        bestFeatureCoefficients = bestEstimator[-1].feature_importances_\n",
    "        coefDf = pd.DataFrame(\n",
    "            bestFeatureCoefficients,\n",
    "            columns=[\"coefficient\"],\n",
    "            index=feature_names\n",
    "        ).reset_index()\n",
    "        return coefDf\n",
    "    \n",
    "    request4 = createRequestFromTemplate(REQUEST_TEMPLATE, 'RandomForestRegressor', pipeline4, parameters4, getImportantFeatures4)\n",
    "    modelReportDf, featureImportanceReportDf = runFeatureSelectionAnalysis(request4, experimentDf, modelReportDf, featureImportanceReportDf)\n",
    "\n",
    "writeDataFrame2Excel(modelReportDf, STEP03_MODELING + 'model_report.')\n",
    "writeDataFrame2Excel(featureImportanceReportDf, STEP03_MODELING + 'feature_report.')\n",
    "dataFrame2Image(modelReportDf, RESULT_FILE_PREFIX + STEP03_MODELING + 'performance.png', height_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def showFeatureEvaluationBarPlots(request, subPlots, pathSuffix, subPlotCols = 3, width=1200, height=400):\n",
    "    filePrefix = RESULT_FILE_PREFIX + STEP04_EVALUATION\n",
    "    labels = request['labels']\n",
    "    numSubPlots = len(subPlots)\n",
    "    \n",
    "    subPlotRows = math.ceil( numSubPlots / subPlotCols)\n",
    "    palette='pastel'\n",
    "    barWidth=0.8\n",
    "    widthInches = width/100\n",
    "    heightInches = height/100\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(subPlotRows, subPlotCols, figsize=(widthInches, heightInches))\n",
    "    subPlotRow = 0\n",
    "    subPlotCol = 0\n",
    "    for subPlot in subPlots:\n",
    "        data = subPlot['data']\n",
    "        xColumn = subPlot['xColumn']\n",
    "        yColumn = subPlot['yColumn']\n",
    "        xLogScale = False\n",
    "        if 'xLogScale' in subPlot:\n",
    "            xLogScale = subPlot['xLogScale']\n",
    "        yLogScale = False\n",
    "        if 'yLogScale' in subPlot:\n",
    "            yLogScale = subPlot['yLogScale']\n",
    "        xAxesLabel = labels[xColumn]\n",
    "        if xLogScale:\n",
    "            xAxesLabel += ' (Log Scale)'\n",
    "        yAxesLabel = labels[yColumn]\n",
    "        if yLogScale:\n",
    "            yAxesLabel += ' (Log Scale)'\n",
    "        title = yAxesLabel + ' vs ' + xAxesLabel\n",
    "        if 'title' in subPlot:\n",
    "            title = subPlot['title']\n",
    "        \n",
    "        width=min(data.shape[0]/2, 1200)\n",
    "        height=8\n",
    "        legendLocation='lower left'\n",
    "        bboxAnchor=(0, -0.5)\n",
    "        \n",
    "        axSub = sns.barplot(ax=ax[subPlotRow, subPlotCol],data=data, x=xColumn, y=yColumn,\n",
    "                         palette=palette, width=barWidth)\n",
    "        axSub.set_title(title)\n",
    "        axSub.set(xlabel=xAxesLabel, ylabel=yAxesLabel)\n",
    "        for bars_group in axSub.containers: \n",
    "            axSub.bar_label(bars_group, padding=5, fontsize=6, label_type='center')\n",
    "        if yLogScale:\n",
    "            plt.yscale('log')\n",
    "        if xLogScale:\n",
    "            plt.xscale('log')\n",
    "        #sns.move_legend(ax, legendLocation, bbox_to_anchor=bboxAnchor)\n",
    "        #ax.figure.set_size_inches(width,height)\n",
    "\n",
    "        subPlotCol +=1\n",
    "        if subPlotCol % subPlotCols == 0:\n",
    "            subPlotRow += 1\n",
    "            subPlotCol = 0\n",
    "\n",
    "    # Get rid of extra plots\n",
    "    if subPlotCol % subPlotCols == 0:\n",
    "            subPlotRow += 1\n",
    "            subPlotCol = 0\n",
    "    \n",
    "    for i in range(subPlotCol, subPlotCols):\n",
    "        fig.delaxes(ax[subPlotRow][i])\n",
    "        \n",
    "    plt.savefig(filePrefix + pathSuffix, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def graphFeatureCoefficients(request, modelPerformanceReportDf, featureCoefficientReportDf, categoryFeatureSets):\n",
    "    filePrefix = RESULT_FILE_PREFIX + STEP04_EVALUATION\n",
    "\n",
    "    # Sort we can find best performer base on TestMSE\n",
    "    modelPerfDf = modelPerformanceReportDf.sort_values('Test MSE', ascending=True)\n",
    "    bestPerformer = modelPerfDf.iloc[0]['Model']\n",
    "    \n",
    "\n",
    "    # All coefficients across all models. Copy so we can change\n",
    "    featureImportanceDf = featureCoefficientReportDf\n",
    "    featureImportanceDf['coefficient_abs'] = featureImportanceDf['coefficient'].abs()\n",
    "    featureImportanceDf = featureImportanceDf.sort_values('coefficient_abs', ascending=False)\n",
    "\n",
    "    # Show all model performances\n",
    "    xColumn = 'coefficient'\n",
    "    colorColumn = 'Model'\n",
    "    yColumn = 'index'\n",
    "    fileSuffix = 'coefficient.png'\n",
    "    showBarPlot(request=request, data=featureImportanceDf, xColumn=xColumn, yColumn=yColumn, colorColumn=colorColumn, \n",
    "                path=filePrefix + fileSuffix, height=(featureImportanceDf.shape[0] * 30), width=2400)\n",
    "\n",
    "\n",
    "    # Show winner\n",
    "    bestModelFeatureImportanceDf = featureImportanceDf.loc[featureImportanceDf['Model'] == bestPerformer]\n",
    "    subPlotsPositiveFeatures = []\n",
    "    subPlotsNegativeFeatures = []\n",
    "    xColumn = 'coefficient_abs'\n",
    "    for key, value in categoryFeatureSets.items():\n",
    "        features = value\n",
    "        for f in features:\n",
    "            # Copy so we can add more columns\n",
    "            featureDf = bestModelFeatureImportanceDf.loc[bestModelFeatureImportanceDf['index'].str.startswith(f)].copy()\n",
    "            featureDf['index'] = featureDf['index'].str.replace(f + '_', '')\n",
    "            posDf = featureDf.loc[featureDf['coefficient'] > 0]\n",
    "            negDf = featureDf.loc[featureDf['coefficient'] <= 0]\n",
    "            subPlotsPositiveFeatures.append(\n",
    "                {\n",
    "                    'data': posDf,\n",
    "                    'title': f'Positive {f} Feature Values',\n",
    "                    'xColumn': xColumn,\n",
    "                    'xLogScale': True,\n",
    "                    'yColumn': yColumn\n",
    "                }\n",
    "            )\n",
    "            subPlotsNegativeFeatures.append(\n",
    "                {\n",
    "                    'data': negDf,\n",
    "                    'title': f'Negative {f} Feature Values',\n",
    "                    'xColumn': xColumn,\n",
    "                    'xLogScale': True,\n",
    "                    'yColumn': yColumn\n",
    "                }\n",
    "            )\n",
    "    showFeatureEvaluationBarPlots(request=request, subPlots=subPlotsPositiveFeatures, pathSuffix='pos.coeff.png', width=2800, height=2400)\n",
    "    showFeatureEvaluationBarPlots(request=request, subPlots=subPlotsNegativeFeatures, pathSuffix='neg.coeff.png', width=2800, height=2400)\n",
    "\n",
    "request = {\n",
    "    'labels': {\n",
    "        'coefficient': 'Coefficent',\n",
    "        'coefficient_abs': 'Coefficent',\n",
    "        'feature_value': 'Feature Value',\n",
    "        'index': 'Feature'\n",
    "    }\n",
    "}\n",
    "featureSets = {\n",
    "    'vehicle': ['manufacturer','type', 'size', 'drive', 'transmission', 'cylinders', 'fuel'],\n",
    "    'ownership': ['condition', 'title_status'],\n",
    "    'location': ['state']\n",
    "}\n",
    "graphFeatureCoefficients(request, modelReportDf, featureImportanceReportDf, featureSets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
